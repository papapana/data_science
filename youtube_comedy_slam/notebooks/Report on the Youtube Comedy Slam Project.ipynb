{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feasibility of the Youtube Comedy Slam Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Initial experiments and exploration of the data showed some promise for being able to decide which video of 2 is funnier. The exploration can be found [here](https://github.com/papapana/data_science/blob/master/youtube_comedy_slam/notebooks/Youtube%20Data%20Exploration.ipynb). The dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data). \n",
    "\n",
    "The dataset contains information about which video is funnier for many pairs of videos. In order to builld a general model that can distinguish between any video which one is funnier we create a score for each video based on our information and later we try to model the score so that videos can be compared.\n",
    "\n",
    "In this notebook we show how to build such a score using multiple ways, and show how all these fail to get an acceptable score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import statsmodels.formula.api as st\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy import io\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have training and test sets given by the dataset itself.\n",
    "The video_score_train is for each video `#funnier_than_other_videos - #not_funnier_than_other_videos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id1</th>\n",
       "      <th>video_id2</th>\n",
       "      <th>funnier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sNabaB-eb3Y</td>\n",
       "      <td>wHkPb68dxEw</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sNabaB-eb3Y</td>\n",
       "      <td>y2emSXSE-N4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vr4D8xO2lBY</td>\n",
       "      <td>sNabaB-eb3Y</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sNabaB-eb3Y</td>\n",
       "      <td>dDtRnstrefE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vX95JgKGu0o</td>\n",
       "      <td>wu4SU70w7LA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     video_id1    video_id2 funnier\n",
       "0  sNabaB-eb3Y  wHkPb68dxEw       0\n",
       "1  sNabaB-eb3Y  y2emSXSE-N4       0\n",
       "2  Vr4D8xO2lBY  sNabaB-eb3Y       1\n",
       "3  sNabaB-eb3Y  dDtRnstrefE       0\n",
       "4  vX95JgKGu0o  wu4SU70w7LA       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_ds_train = pd.read_csv('../data/processed/comedy_comparisons.train')\n",
    "initial_ds_test = pd.read_csv('../data/processed/comedy_comparisons.test')\n",
    "initial_ds_train.funnier[initial_ds_train.funnier == 'left'] = 0\n",
    "initial_ds_train.funnier[initial_ds_train.funnier == 'right'] = 1\n",
    "initial_ds_test.funnier[initial_ds_test.funnier == 'left'] = 0\n",
    "initial_ds_test.funnier[initial_ds_test.funnier == 'right'] = 1\n",
    "video_score_train = pickle.load(open('../data/processed/video_score_train.p', 'rb'))\n",
    "video_score_test = pickle.load(open('../data/processed/video_score_test.p', 'rb'))\n",
    "initial_ds_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_initial_problem(initial_dataset, video_score_test):\n",
    "    bin_ytest = []\n",
    "    for row in initial_dataset.itertuples():\n",
    "        pred1 = video_score_test[row.video_id1] if row.video_id1 in video_score_test else 0.0\n",
    "        pred2 = video_score_test[row.video_id2] if row.video_id2 in video_score_test else 0.0\n",
    "        bin_ytest += [0 if pred1 >= pred2 else 1]\n",
    "    return bin_ytest\n",
    "\n",
    "y_pred_train = predict_initial_problem(initial_ds_train, video_score_train)\n",
    "y_pred_test = predict_initial_problem(initial_ds_test, video_score_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.65      0.55     18884\n",
      "          1       0.49      0.32      0.39     19713\n",
      "\n",
      "avg / total       0.48      0.48      0.47     38597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[12361  6523]\n",
      " [13439  6274]]\n",
      "Accuracy: 0.482809544783\n"
     ]
    }
   ],
   "source": [
    "y_train = list(initial_ds_train.funnier.tolist())\n",
    "y_test = list(initial_ds_test.funnier.tolist())\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred_test))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy of the first scoring system is only 48% which is worse than chance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the problem as a graph\n",
    "\n",
    "In this section we model the dataset as a directed graph where the nodes are videos and the edges are directed to the funnier of the 2 videos.\n",
    "\n",
    "We use the library networkx to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_ds_as_graph(ds):\n",
    "    G = nx.DiGraph()\n",
    "    for row in ds.itertuples():\n",
    "        G.add_edges_from([(row.video_id1, row.video_id2)] if row.funnier else [(row.video_id2, row.video_id1)])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "g_train = get_ds_as_graph(initial_ds_train)\n",
    "g_test = get_ds_as_graph(initial_ds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of the graph and their meaning\n",
    "\n",
    "First of all, we want to check if in the dataset the transitive property holds.\n",
    "That is if video A is funnier than video B and B is funnier than C, then A is funnier than C.\n",
    "In our graph this would mean that there is an edge $A\\leftarrow B$ and $B\\leftarrow C$  but not $C\\leftarrow A$. This would create a cycle and break the transitive property.\n",
    "\n",
    "So, we want the graph to be a directed acyclic graph (DAG) so that we can use comparisons in a consistent way.\n",
    "\n",
    "We test both the train and test graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed_acyclic_graph(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed_acyclic_graph(g_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they are not DAGs which is bad for creating a score where we are sure about the hierarchy of videos.\n",
    "\n",
    "Then we proceed to check if we have all the relations we need i.e. every video is related to every other through some intermediate videos e.g. if we want to see if video A is funnier than Z then there is some connection `path` from A to Z. This would be translated in the video graph if it is strongly connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_strongly_connected(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_strongly_connected(g_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs are not strongly connected and therefore we cannot make claims for every pair of videos.\n",
    "\n",
    "We now check if at least there is some connection of the videos without taking into account direction (which one is funnier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_weakly_connected_components(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.number_weakly_connected_components(g_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of weakly connected components is not 1 there is no connection between every 2 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now check if we score the importance of each node using this [network clustering algorithm](https://networkx.github.io/documentation/networkx-1.10/reference/generated/networkx.algorithms.cluster.clustering.html#networkx.algorithms.cluster.clustering) if we get more predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.54      0.51     18884\n",
      "          1       0.51      0.47      0.49     19713\n",
      "\n",
      "avg / total       0.50      0.50      0.50     38597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[10120  8764]\n",
      " [10487  9226]]\n",
      "Accuracy: 0.501230665596\n"
     ]
    }
   ],
   "source": [
    "g_test_undirected = g_test.to_undirected()\n",
    "video_score_test_clustered = nx.clustering(g_test_undirected)\n",
    "y_pred_test_clustered = predict_initial_problem(initial_ds_test, video_score_test_clustered)\n",
    "print(metrics.classification_report(y_test, y_pred_test_clustered))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test_clustered))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test_clustered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still we are at 50% and therefore this is not a good scoring system either for this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try the [pagerank algorithm](https://en.wikipedia.org/wiki/PageRank) that is a good algorithm for assigning weights to nodes of a directed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.51      0.50     18884\n",
      "          1       0.51      0.49      0.50     19713\n",
      "\n",
      "avg / total       0.50      0.50      0.50     38597\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 9603  9281]\n",
      " [10003  9710]]\n",
      "Accuracy: 0.500375676866\n"
     ]
    }
   ],
   "source": [
    "pr = nx.pagerank(g_test, alpha=0.9)\n",
    "y_pred_test_pagerank = predict_initial_problem(initial_ds_test, pr)\n",
    "print(metrics.classification_report(y_test, y_pred_test_pagerank))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred_test_pagerank))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test_pagerank))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stil we have very bad accuracy.\n",
    "We now try tuning the pagerank algorithm using different $\\alpha$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50037567686607765"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "for alpha in np.arange(0.1, 0.9, 0.01):\n",
    "    pr = nx.pagerank(g_test, alpha=0.9)\n",
    "    y_pred_test_pagerank = predict_initial_problem(initial_ds_test, pr)\n",
    "    scores += [metrics.accuracy_score(y_true=y_test, y_pred=y_pred_test_pagerank)]\n",
    "max(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the maximum accuracy we obtain from pagerank importance of the node is still 50% which could have been chance.\n",
    "\n",
    "We conclude that with this dataset is difficult to rank the importance of each node and we need to look even further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current research on the dataset\n",
    "\n",
    "In fact there is some relevant research on the subject from Baluja, S (2016) where a more complicated method than PageRank is applied, called Adsorption and gets a score of 62%.\n",
    "\n",
    "## Conclusion\n",
    "To sum up, we tried to invent a score for the videos that can predict which one is funnier for the youtube comedy slam dataset so that we can create a more general model. We tried first a na√Øve scoring model, graph clustering, pagerank and then referred to state-of-the-art research on the subject. Additionally, we examined properties of the video funniness graph and showed that it lacks many desirable properties. Therefore, it is not a promising dataset to continue examining it\n",
    "\n",
    "### References\n",
    "\n",
    "`Baluja, S. (2016). A Simple and Efficient Method to Handle Sparse Preference Data Using Domination Graphs: An Application to YouTube. Procedia Computer Science, 80, 2302-2311.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
