{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube comedy slam project report\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "Given a pair of youtube videos, the objective is to determine which one is funnier using the comments of the users. \n",
    "\n",
    "This can have various applications and provide several opportunities for monetization. For example, for Youtube itself the knowledge of which video is funnier can help in strategic decision making for allocating advertisements. Additionally, it can provide an insight on what specific audiences consider more entertaining and make more investments for the creation of particular content types. Moreover, the same models could possibly be applied in other contexts to understand which content has more value depending on the comments of users. Therefore, the client could be Youtube itself, investors in content creation, advertisers or even comedians or researchers in what is considered funny.\n",
    "\n",
    "# Dataset\n",
    "The dataset consists of 912969 video pairs in the training set and 225593 in the test set. For each pair the additional information of which video of the pair is funnier, is provided. This dataset has been made available publicly by Google and can be found [here](https://archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data).\n",
    "\n",
    "## Challenges of data acquisition and cleaning\n",
    "The dataset mentioned above contains only the video pairs and not the actual comments that are needed to perform the data analysis and therefore the comments have to be downloaded. One of the first challenges were that some videos of the dataset do not exist anymore or have become private. So,  The problem is that Google blocks spidering in quick succession after about the 135th video downloaded and spidering is impervious to IP change. I resorted in learning and using the official Youtube API for downloading comments. The Youtube API also has constraints on how many requests one can do per day.\n",
    "\n",
    "# Solution Outline\n",
    "This problem is a classification problem. Therefore we are going to define a training, a cross-validation and a test-set. We are going to collect as much data as possible (comments). \n",
    "\n",
    "Firstly, we are going to explore the data by creating and answering many questions about the dataset in order to get a good intuition about the data.\n",
    "Then, we are going to apply increasingly more complex algorithms for classification and do error analysis using precision, recall and f1 scores. Then we will manually examine the errors made in the cross-validation set and propose new features to improve the classification (e.g. stemming, upper vs lower case).\n",
    "\n",
    "One of the purposes of the project is also to learn and experiment with deep learning for NLP.\n",
    "\n",
    "# Deliverables\n",
    "The deliverables will include code files, at least one python notebook documenting all the procedure to reproduce the results, a paper documenting the most important aspects of the project and presentation slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
